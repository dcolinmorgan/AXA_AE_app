{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcolinmorgan/AXA_AE_app/blob/main/axa_ae_gcn_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6yuGrb914f"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akNbX1yL914f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import typing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "lHrFQ1uZ-YSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Measure Data"
      ],
      "metadata": {
        "id": "Y2XNcajyYP9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://github.com/dcolinmorgan/AXA_AE_app/blob/main/AE_AXA_poll-ae.parquet?raw=true\n",
        "data=pd.read_parquet('/content/AE_AXA_poll-ae.parquet?raw=true')\n",
        "\n",
        "# !wget https://github.com/dcolinmorgan/AXA_AE_app/blob/main/edmond_datasets.pickle?raw=true\n",
        "# data=pd.read_pickle('edmond_datasets.pickle?raw=true')"
      ],
      "metadata": {
        "id": "KYbgGW9O-7ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_parquet('/content/AE_AXA_poll-ae.parquet?raw=true')\n",
        "# load dataset\n",
        "# dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
        "# data=pd.read_parquet('~/run/AXA_AE_app/AE_AXA_poll-ae.parquet')\n",
        "data['loca']=data.groupby('cd9_loc').ngroup()\n",
        "data2=pd.DataFrame()\n",
        "for loc in np.unique(data.loca):\n",
        "    # data=pd.read_parquet('~/run/AXA_AE_app/AE_AXA_poll-ae.parquet')\n",
        "    \n",
        "    datum=data[data['loca']==loc]\n",
        "    datum=datum.groupby(['loca','date','cd9_loc']).agg('mean')[['pm25','pm10','o3','no2','so2','co','age','sex','diag1']]\n",
        "    datum.reset_index(inplace=True)\n",
        "    idx = pd.date_range(min(data['date']), max(data['date']))\n",
        "    datum.index = pd.DatetimeIndex(datum['date'])#.fillna()\n",
        "\n",
        "    # s.index = pd.DatetimeIndex(s.index)\n",
        "\n",
        "    datum = datum.reindex(idx, fill_value=np.nan)\n",
        "    datum['date']=datum.index\n",
        "    datum['loca']=loc #fillna(method='ffill',inplace=True)\n",
        "    if datum['pm25'].isna().sum() < 2922/1.5: \n",
        "        data2=pd.concat([data2,datum],axis=0)\n",
        "# data2['date']=data2.index\n",
        "data=data2\n",
        "# data['loca'].fillna(method='ffill',inplace=True)\n",
        "# data.reset_index(inplace=True)\n",
        "# data['loca']=data.groupby('cd9_loc').ngroup()\n",
        "data['date']=(data['date']-data['date'][0]).dt.days\n",
        "\n",
        "data=data[['loca','cd9_loc','date','pm25','pm10','o3','no2','so2','co','age','sex','diag1']]\n",
        "data=data.groupby(['loca','cd9_loc','date']).agg('mean')[['pm25','pm10','o3','no2','so2','co','age','sex','diag1']]\n",
        "\n",
        "data.age=np.round(data.age)\n",
        "data.sex=np.round(data.sex)\n",
        "data.sort_values(by=['loca','date'],inplace=True)\n",
        "first_column = data.pop('diag1')\n",
        "data.insert(0, 'diag1', first_column)\n",
        "data.reset_index(inplace=True)\n",
        "data.fillna(method='bfill',inplace=True)\n",
        "data.fillna(method='ffill',inplace=True)\n",
        "\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = data.drop(['cd9_loc'],axis=1).values #scaler.fit_transform(data.drop(['cd9_loc'],axis=1))\n",
        "cc=data['cd9_loc']\n",
        "# dataset = data.drop(['cd9_loc'],axis=1).values\n",
        "data=pd.DataFrame(dataset)\n",
        "data.columns=['loca','date','diag1','pm25','pm10','o3','no2','so2','co','age','sex']\n",
        "data['cd9_loc']=cc\n",
        "\n",
        "# specify columns to plot\n",
        "groups = [0, 1,2, 3, 4, 5, 6, 7, 8,9,10]\n",
        "i = 1\n",
        "# plot each column\n",
        "plt.figure()\n",
        "for group in groups:\n",
        "\tplt.subplot(len(groups), 1, i)\n",
        "\tplt.plot(dataset[:, group])\n",
        "\tplt.title(data.drop('cd9_loc',axis=1).columns[group], y=0.5, loc='right')\n",
        "\ti += 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jL2IiUAT_ReS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph data"
      ],
      "metadata": {
        "id": "D3sZZXBUYVAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopy.distance\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"example app\")\n",
        "df_loc=pd.DataFrame(columns=['lat','long','name'])\n",
        "for ii,i in enumerate(pd.unique(data['cd9_loc'])):\n",
        "    a,b,c=geolocator.geocode(str(i)+\", Hong Kong\").point\n",
        "    df_loc[ii]=[a,b,i]\n",
        "df_loc=df_loc.transpose()\n",
        "df_loc.columns=['lat','long','name']\n",
        "df_loc=df_loc[3:]"
      ],
      "metadata": {
        "id": "DnuiFSa9_TcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loc=df_loc\n",
        "data_loc['latlon'] = list(zip(data_loc['lat'], data_loc['long']))\n",
        "\n",
        "square = pd.DataFrame(\n",
        "    np.zeros((data_loc.shape[0], data_loc.shape[0])),\n",
        "    index=data_loc.index, columns=data_loc.index\n",
        ")\n",
        "\n",
        "def get_distance(col):\n",
        "    end = data_loc.loc[col.name, 'latlon']\n",
        "    return data_loc['latlon'].apply(geopy.distance.distance,\n",
        "                              args=(end,),\n",
        "                              ellipsoid='WGS-84'\n",
        "                             )\n",
        "distances = square.apply(get_distance, axis=1).T\n",
        "\n",
        "data_loc['src']=data_loc['name']\n",
        "data_loc['dst']=data_loc['name']\n",
        "\n",
        "# np.sum((distances<5)*1)\n",
        "D_D=pd.DataFrame((distances<5)*1)\n",
        "D_D.index=data_loc['src']\n",
        "D_D.columns=data_loc['dst']\n",
        "\n",
        "E_E=pd.DataFrame(D_D.stack())#.reset_index(inplace=True)\n",
        "# E_E.rename=['source','target']#.reset_index(inplace=True)#.rename(columns={'level_0':'Source','level_1':'Target', 0:'Weight'})\n",
        "E_E.reset_index(inplace=True)#\n",
        "distance_mat=E_E[E_E[0]>0]\n",
        "\n",
        "distance=distances\n",
        "distance.index=data_loc['src']\n",
        "distance.columns=data_loc['dst']\n",
        "distance=pd.DataFrame(distance.stack())\n",
        "distance.reset_index(inplace=True)\n",
        "\n",
        "#prepare for TF\n",
        "\n",
        "distances=distances.astype(str) # df.astype(np.float64)#lues.as_int#('int')#.to_numpy()\n",
        "distances=distances.replace('km', '', regex=True)\n",
        "distances=distances.astype(np.float64)\n",
        "distances.shape\n"
      ],
      "metadata": {
        "id": "r3afKKE0_VFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_adjacency_matrix(\n",
        "    route_distances: np.ndarray, sigma2: float, epsilon: float\n",
        "):\n",
        "    \"\"\"Computes the adjacency matrix from distances matrix.\n",
        "\n",
        "    It uses the formula in https://github.com/VeritasYin/STGCN_IJCAI-18#data-preprocessing to\n",
        "    compute an adjacency matrix from the distance matrix.\n",
        "    The implementation follows that paper.\n",
        "\n",
        "    Args:\n",
        "        route_distances: np.ndarray of shape `(num_routes, num_routes)`. Entry `i,j` of this array is the\n",
        "            distance between roads `i,j`.\n",
        "        sigma2: Determines the width of the Gaussian kernel applied to the square distances matrix.\n",
        "        epsilon: A threshold specifying if there is an edge between two nodes. Specifically, `A[i,j]=1`\n",
        "            if `np.exp(-w2[i,j] / sigma2) >= epsilon` and `A[i,j]=0` otherwise, where `A` is the adjacency\n",
        "            matrix and `w2=route_distances * route_distances`\n",
        "\n",
        "    Returns:\n",
        "        A boolean graph adjacency matrix.\n",
        "    \"\"\"\n",
        "    num_routes = route_distances.shape[0]\n",
        "    route_distances = route_distances / 10000.0\n",
        "    w2, w_mask = (\n",
        "        route_distances * route_distances,\n",
        "        np.ones([num_routes, num_routes]) - np.identity(num_routes),\n",
        "    )\n",
        "    return (np.exp(-w2 / sigma2) >= epsilon) * w_mask"
      ],
      "metadata": {
        "id": "GwXz51kfoEbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HDYGcC_914l"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphInfo:\n",
        "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
        "        self.edges = edges\n",
        "        self.num_nodes = num_nodes\n",
        "\n",
        "\n",
        "sigma2 = 0.1\n",
        "epsilon = .99998\n",
        "adjacency_matrix = compute_adjacency_matrix(distances, sigma2, epsilon)\n",
        "\n",
        "# distances=np.random.permutation(distances)\n",
        "\n",
        "# adjacency_matrix =np.where(distances > 15, 0, 1)\n",
        "\n",
        "# adjacency_matrix=np.ones(distances.shape)\n",
        "\n",
        "node_indices, neighbor_indices = np.where(adjacency_matrix == 1)\n",
        "graph = GraphInfo(\n",
        "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
        "    num_nodes=adjacency_matrix.shape[0],\n",
        ")\n",
        "print(f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.matshow((adjacency_matrix))"
      ],
      "metadata": {
        "id": "fjZdZ1p_Ba7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM data"
      ],
      "metadata": {
        "id": "keYIuxgY83WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# measures_array=data.pivot_table(values=['pm25','pm10','o3','no2','so2','co'], index='date', columns=['loca'], aggfunc='mean').fillna(method='bfill')\n",
        "# measures_array=measures_array.dropna().values\n",
        "# measures_array=measures_array.reshape([measures_array.shape[0],14,6])\n",
        "# measures_array.shape"
      ],
      "metadata": {
        "id": "QhEydXUcjdCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "measures_array=data.pivot_table(values=['pm25','pm10','o3','no2','co','so2','age','sex'], index='date', columns=['loca'], aggfunc='mean').fillna(method='bfill')\n",
        "measures_array=measures_array.dropna().values\n",
        "measures_array=measures_array.reshape([measures_array.shape[0],8,14])\n",
        "measures_array=np.transpose(measures_array, [0, 2, 1])\n",
        "# cc=measures_array[:,:,2]\n",
        "# measures_array[:,:,0]=measures_array[:,:,2] ## move diag1 to 0 index\n",
        "# measures_array[:,:,2]=cc\n",
        "measures_array.shape ##age will be predicted here"
      ],
      "metadata": {
        "id": "DHr8uafD_v9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceDC458-bT05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(measures_array[:,:,0]) ## check diag1 is 0 index 3rd dim"
      ],
      "metadata": {
        "id": "xSLjHc5ngH_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measures_array=data.pivot_table(values=['pm10'], index='date', columns=['loca'], aggfunc='mean').fillna(method='bfill').values\n",
        "# measures_array.shape"
      ],
      "metadata": {
        "id": "kCXWNp86SqLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isfinite(measures_array).all()\n"
      ],
      "metadata": {
        "id": "45ujMW9a28bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BWgJYAF914j"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "plt.plot(measures_array[:,1])\n",
        "# plt.legend([\"route_0\", \"route_25\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6nqnNEn914k"
      },
      "outputs": [],
      "source": [
        "train_size, val_size = 0.5, 0.2\n",
        "\n",
        "\n",
        "def preprocess(data_array: np.ndarray, train_size: float, val_size: float):\n",
        "\n",
        "    num_time_steps = data_array.shape[0]\n",
        "    num_train, num_val = (\n",
        "        int(num_time_steps * train_size),\n",
        "        int(num_time_steps * val_size),\n",
        "    )\n",
        "    train_array = data_array[:num_train]\n",
        "    mean, std = train_array.mean(axis=0), train_array.std(axis=0)\n",
        "\n",
        "    train_array = (train_array )#- mean) / std\n",
        "    val_array = (data_array[num_train : (num_train + num_val)])# - mean) / std\n",
        "    test_array = (data_array[(num_train + num_val) :])# - mean) / std\n",
        "\n",
        "    return train_array, val_array, test_array\n",
        "\n",
        "\n",
        "train_array, val_array, test_array = preprocess(measures_array, train_size, val_size)\n",
        "\n",
        "print(f\"train set size: {train_array.shape}\")\n",
        "print(f\"validation set size: {val_array.shape}\")\n",
        "print(f\"test set size: {test_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isfinite(train_array).all()\n",
        "# np.isfinite(b_0).all()"
      ],
      "metadata": {
        "id": "2y05LFlN2wAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9iM-KSe914m"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphConv(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_feat,\n",
        "        out_feat,\n",
        "        graph_info: GraphInfo,\n",
        "        aggregation_type=\"mean\",\n",
        "        combination_type=\"concat\",\n",
        "        activation: typing.Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.in_feat = in_feat\n",
        "        self.out_feat = out_feat\n",
        "        self.graph_info = graph_info\n",
        "        self.aggregation_type = aggregation_type\n",
        "        self.combination_type = combination_type\n",
        "        self.weight = tf.Variable(\n",
        "            initial_value=keras.initializers.glorot_uniform()(\n",
        "                shape=(in_feat, out_feat), dtype=\"float32\"\n",
        "            ),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.activation = layers.Activation(activation)\n",
        "\n",
        "    def aggregate(self, neighbour_representations: tf.Tensor):\n",
        "        aggregation_func = {\n",
        "            \"sum\": tf.math.unsorted_segment_sum,\n",
        "            \"mean\": tf.math.unsorted_segment_mean,\n",
        "            \"max\": tf.math.unsorted_segment_max,\n",
        "        }.get(self.aggregation_type)\n",
        "\n",
        "        if aggregation_func:\n",
        "            return aggregation_func(\n",
        "                neighbour_representations,\n",
        "                self.graph_info.edges[0],\n",
        "                num_segments=self.graph_info.num_nodes,\n",
        "            )\n",
        "\n",
        "        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n",
        "\n",
        "    def compute_nodes_representation(self, features: tf.Tensor):\n",
        "        \"\"\"Computes each node's representation.\n",
        "\n",
        "        The nodes' representations are obtained by multiplying the features tensor with\n",
        "        `self.weight`. Note that\n",
        "        `self.weight` has shape `(in_feat, out_feat)`.\n",
        "\n",
        "        Args:\n",
        "            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
        "        \"\"\"\n",
        "        return tf.matmul(features, self.weight)\n",
        "\n",
        "    def compute_aggregated_messages(self, features: tf.Tensor):\n",
        "        neighbour_representations = tf.gather(features, self.graph_info.edges[1])\n",
        "        aggregated_messages = self.aggregate(neighbour_representations)\n",
        "        return tf.matmul(aggregated_messages, self.weight)\n",
        "\n",
        "    def update(self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor):\n",
        "        if self.combination_type == \"concat\":\n",
        "            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n",
        "        elif self.combination_type == \"add\":\n",
        "            h = nodes_representation + aggregated_messages\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
        "\n",
        "        return self.activation(h)\n",
        "\n",
        "    def call(self, features: tf.Tensor):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n",
        "        \"\"\"\n",
        "        nodes_representation = self.compute_nodes_representation(features)\n",
        "        aggregated_messages = self.compute_aggregated_messages(features)\n",
        "        return self.update(nodes_representation, aggregated_messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkHEtWYg914m"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMGC(layers.Layer):\n",
        "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_feat,\n",
        "        out_feat,\n",
        "        lstm_units: int,\n",
        "        input_seq_len: int,\n",
        "        output_seq_len: int,\n",
        "        graph_info: GraphInfo,\n",
        "        graph_conv_params: typing.Optional[dict] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # graph conv layer\n",
        "        if graph_conv_params is None:\n",
        "            graph_conv_params = {\n",
        "                \"aggregation_type\": \"mean\",\n",
        "                \"combination_type\": \"concat\",\n",
        "                \"activation\": None,\n",
        "            }\n",
        "        self.graph_conv = GraphConv(in_feat, out_feat, graph_info, **graph_conv_params)\n",
        "\n",
        "        self.lstm = layers.LSTM(lstm_units, activation=\"relu\")#,return_sequences=True)\n",
        "        self.dense = layers.Dense(output_seq_len)\n",
        "        # self.dense = layers.Dense(1)\n",
        "\n",
        "        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n",
        "        \"\"\"\n",
        "\n",
        "        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n",
        "        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n",
        "\n",
        "        gcn_out = self.graph_conv(\n",
        "            inputs\n",
        "        )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n",
        "        shape = tf.shape(gcn_out)\n",
        "        num_nodes, batch_size, input_seq_len, out_feat = (\n",
        "            shape[0],\n",
        "            shape[1],\n",
        "            shape[2],\n",
        "            shape[3],\n",
        "        )\n",
        "\n",
        "        # LSTM takes only 3D tensors as input\n",
        "        gcn_out = tf.reshape(gcn_out, (batch_size * num_nodes, input_seq_len, out_feat))\n",
        "        lstm_out = self.lstm(\n",
        "            gcn_out\n",
        "        )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n",
        "\n",
        "        dense_output = self.dense(\n",
        "            lstm_out\n",
        "        )  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n",
        "        output = tf.reshape(dense_output, (num_nodes, batch_size, self.output_seq_len))\n",
        "        return tf.transpose(\n",
        "            output, [1, 2, 0]\n",
        "        )  # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TeRR3ZG914l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
        "\n",
        "batch_size = 64\n",
        "input_sequence_length = 21\n",
        "forecast_horizon = 7\n",
        "multi_horizon = True\n",
        "\n",
        "\n",
        "def create_tf_dataset(\n",
        "    data_array: np.ndarray,\n",
        "    input_sequence_length: int,\n",
        "    forecast_horizon: int,\n",
        "    batch_size: int = 128,\n",
        "    shuffle=True,\n",
        "    multi_horizon=True,\n",
        "):\n",
        "    if data_array.shape[2] ==1:\n",
        "      inputs = timeseries_dataset_from_array(\n",
        "          np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n",
        "          None,\n",
        "          sequence_length=input_sequence_length,\n",
        "          shuffle=False,\n",
        "          batch_size=batch_size,\n",
        "      )\n",
        "\n",
        "    else:\n",
        "      inputs = timeseries_dataset_from_array(\n",
        "          data_array[:-forecast_horizon,:,1:],\n",
        "          None,\n",
        "          sequence_length=input_sequence_length,\n",
        "          shuffle=False,\n",
        "          batch_size=batch_size,\n",
        "      )\n",
        "\n",
        "    target_offset = (\n",
        "        input_sequence_length\n",
        "        if multi_horizon\n",
        "        else input_sequence_length + forecast_horizon - 1\n",
        "    )\n",
        "    target_seq_length = forecast_horizon if multi_horizon else 1\n",
        "    targets = timeseries_dataset_from_array(\n",
        "        data_array[target_offset:,:,0],\n",
        "        # data_array[:,:,0],\n",
        "        # np.expand_dims(data_array[target_offset:,:,0], axis=-1),\n",
        "        None,\n",
        "        sequence_length=target_seq_length,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((inputs, targets))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(100)\n",
        "\n",
        "    return dataset.prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = (\n",
        "    create_tf_dataset(data_array, input_sequence_length, forecast_horizon, batch_size)\n",
        "    for data_array in [train_array, val_array]\n",
        ")\n",
        "\n",
        "test_dataset = create_tf_dataset(\n",
        "    test_array,\n",
        "    input_sequence_length,\n",
        "    forecast_horizon,\n",
        "    batch_size=test_array.shape[0],\n",
        "    shuffle=False,\n",
        "    multi_horizon=multi_horizon,\n",
        ")\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxc-eaL914m"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO91GhEx914m"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "in_feat = val_dataset.element_spec[0].shape[3]\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "# input_sequence_length = \n",
        "# forecast_horizon = \n",
        "multi_horizon = False\n",
        "out_feat = 14\n",
        "lstm_units = 256\n",
        "graph_conv_params = {\n",
        "    \"aggregation_type\": \"mean\",\n",
        "    \"combination_type\": \"concat\",\n",
        "    \"activation\": None,\n",
        "}\n",
        "\n",
        "st_gcn = LSTMGC(\n",
        "    in_feat,\n",
        "    out_feat,\n",
        "    lstm_units,\n",
        "    input_sequence_length,\n",
        "    forecast_horizon,\n",
        "    graph,\n",
        "    graph_conv_params,\n",
        ")\n",
        "inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))\n",
        "outputs = st_gcn(inputs)\n",
        "\n",
        "model = keras.models.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),\n",
        "    loss=keras.losses.MeanAbsoluteError(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")"
      ],
      "metadata": {
        "id": "BT5sYPcmOlB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxUKwxv914m"
      },
      "source": [
        "## Making forecasts on test set\n",
        "\n",
        "Now we can use the trained model to make forecasts for the test set. Below, we\n",
        "compute the MAE of the model and compare it to the MAE of naive forecasts.\n",
        "The naive forecasts are the last value of the measure for each node."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## with randomized graph\n",
        "\n",
        "x_test, y = next(test_dataset.as_numpy_iterator())\n",
        "y_pred = model.predict(x_test)\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.plot(y[:, 0, 0])\n",
        "plt.plot(y_pred[:, 0, 0])\n",
        "plt.legend([\"actual\", \"forecast\"])\n",
        "\n",
        "naive_mse, model_mse = (\n",
        "    # np.square(x_test[:, -1, :, 0] - y[:, 0, :]).mean(),\n",
        "    # np.square(y_pred[:, 0, :] - y[:, 0, :]).mean(),\n",
        "\n",
        "    np.square(np.nanmean(x_test[:, -1, :,0] - y[:, 0, :])),\n",
        "    np.square(np.nanmean(y_pred[:, 0, :] - y[:, 0, :])),\n",
        ")\n",
        "print(f\"naive MSE: {naive_mse}, model MSE: {model_mse}\")"
      ],
      "metadata": {
        "id": "XtbApB6C_l2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OD8s7D3_jqP5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}