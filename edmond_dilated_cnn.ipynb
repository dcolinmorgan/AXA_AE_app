{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-license",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 18:59:53.502809: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-23 18:59:53.507555: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/anaconda3-2021-05/lib:/cm/shared/apps/pbspro-ce/19.1.3/lib/\n",
      "2022-09-23 18:59:53.507573: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# !pip install shap\n",
    "# import shap\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8481d209-123d-4f73-bd9e-fa5598119bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sliding1(X, num_steps_x = 2, num_steps_y = 2):\n",
    "    X = pd.DataFrame(X)\n",
    "    X_transformed = [np.array(X.shift(i)) for i in range(num_steps_x + num_steps_y)]\n",
    "    X_transformed = np.dstack(X_transformed)\n",
    "    \n",
    "    # swap time steps and dimensionality axes\n",
    "    X_transformed = np.swapaxes(X_transformed, 1, 2)\n",
    "    # flip time steps axis\n",
    "    X_transformed = np.flip(X_transformed, 1)\n",
    "    X_transformed = X_transformed[(num_steps_x+num_steps_y - 1):]\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "super-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_EDdata(input_data_shiftedy,var,n_steps_x,n_steps_y):\n",
    "\n",
    "    # number of time steps for sequence learning\n",
    "    # n_steps_x = 14\n",
    "    # number of time steps for sequence prediction\n",
    "    # n_steps_y = 1\n",
    "\n",
    "    split_at = int(len(input_data_shiftedy) * .8)\n",
    "    val_split_at = int(len(input_data_shiftedy) * .9)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # if only prevalence data\n",
    "    input_data_shiftedy = input_data_shiftedy.values.reshape((input_data_shiftedy.shape[0], -1))\n",
    "    input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)] = scaler.fit_transform(\n",
    "        input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)])\n",
    "    # only transform valid and testing sets\n",
    "    input_data_shiftedy[(split_at + n_steps_x + n_steps_y):] = scaler.transform(\n",
    "        input_data_shiftedy[(split_at + n_steps_x + n_steps_y):])\n",
    "\n",
    "\n",
    "    input_data_shiftedy = reshape_sliding1(input_data_shiftedy, \n",
    "                                           num_steps_x = n_steps_x,\n",
    "                                          num_steps_y = n_steps_y)\n",
    "\n",
    "    # train_test_split \n",
    "\n",
    "    X_train = input_data_shiftedy[:split_at, :n_steps_x, :data.shape[1]-1]\n",
    "    X_valid = input_data_shiftedy[split_at:val_split_at, :n_steps_x, :data.shape[1]-1]\n",
    "    X_test = input_data_shiftedy[val_split_at:, :n_steps_x, :data.shape[1]-1]\n",
    "\n",
    "    Y = np.empty((input_data_shiftedy.shape[0], n_steps_x))\n",
    "    for step_ahead in range(1, n_steps_y + 1):\n",
    "        # print(step_ahead, step_ahead + n_steps_x)\n",
    "        Y = input_data_shiftedy[..., step_ahead:step_ahead + n_steps_x, 0]\n",
    "    Y_train = Y[:split_at, data.shape[1]-1:]\n",
    "    Y_valid = Y[split_at:val_split_at,data.shape[1]-1:]\n",
    "    Y_test = Y[val_split_at:,data.shape[1]-1:]\n",
    "\n",
    "    # checking dimensionality\n",
    "    print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\n",
    "    # assert X_train.shape[1:] == X_valid.shape[1:]\n",
    "    # assert Y_train.shape[1:] == Y_valid.shape[1:]\n",
    "\n",
    "    return X_train,Y_train,X_test,Y_test,X_valid,Y_valid,var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cc00ab-d53b-4b82-be1f-a86c5c18eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1861, 14, 6) (1861, 0) (219, 14, 6) (219, 0) (233, 14, 6) (233, 0)\n"
     ]
    }
   ],
   "source": [
    "n_steps_x=14\n",
    "n_steps_y=1\n",
    "\n",
    "with open(\"edmond_datasets.pickle\", \"rb\") as handle:\n",
    "    input_data_shiftedy = pickle.load(handle)\n",
    "# select only n, daily asthma visits and total, daily total AE visits, \n",
    "# comment out if including air pollution station data\n",
    "first_col = input_data_shiftedy.n / input_data_shiftedy.total * 1000\n",
    "input_data_shiftedy.insert(0, \"prevalence\", first_col)\n",
    "input_data_shiftedy = input_data_shiftedy.drop([\"n\", \"total\", \"y\"], axis = 1)\n",
    "data=pd.DataFrame(input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1),columns=['CO'])\n",
    "data['NO2']=input_data_shiftedy.filter(like='NO2_', axis=1).mean(axis=1)\n",
    "data['O3']=input_data_shiftedy.filter(like='O3_', axis=1).mean(axis=1)\n",
    "data['SO2']=input_data_shiftedy.filter(like='SO2_', axis=1).mean(axis=1)\n",
    "data['CO']=input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1)\n",
    "data['FSP']=input_data_shiftedy.filter(like='FSP_', axis=1).mean(axis=1)\n",
    "data[['prevalence','temp','RelHum']]=input_data_shiftedy[['prevalence','Mean (deg. C)','Mean Relative Humidity (%)']]\n",
    "# input_data_shiftedy = pd.concat([input_data_shiftedy.n.reset_index(drop = True), \n",
    "#                         input_data_shiftedy.total.reset_index(drop = True)], axis = 1)\n",
    "# input_data_shiftedy = input_data_shiftedy[var]\n",
    "first_column = data.pop('prevalence')\n",
    "data.insert(0, 'prevalence', first_column)\n",
    "\n",
    "# if i=='prev':\n",
    "#     data=data[['prevalence']]\n",
    "# elif i=='ae':\n",
    "#     data=data[['prevalence','temp','RelHum']]\n",
    "# elif i =='poll':\n",
    "input_data_shiftedy=data[['prevalence','FSP','O3','NO2','SO2','CO']]\n",
    "\n",
    "split_at = int(len(input_data_shiftedy) * .8)\n",
    "val_split_at = int(len(input_data_shiftedy) * .9)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# if only prevalence data\n",
    "input_data_shiftedy = input_data_shiftedy.values.reshape((input_data_shiftedy.shape[0], -1))\n",
    "input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)] = scaler.fit_transform(\n",
    "    input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)])\n",
    "# only transform valid and testing sets\n",
    "input_data_shiftedy[(split_at + n_steps_x + n_steps_y):] = scaler.transform(\n",
    "    input_data_shiftedy[(split_at + n_steps_x + n_steps_y):])\n",
    "\n",
    "\n",
    "input_data_shiftedy = reshape_sliding1(input_data_shiftedy, \n",
    "                                       num_steps_x = n_steps_x,\n",
    "                                      num_steps_y = n_steps_y)\n",
    "\n",
    "data=input_data_shiftedy\n",
    "\n",
    "X_train = data[:split_at, :n_steps_x, :data.shape[1]-1]\n",
    "X_valid = input_data_shiftedy[split_at:val_split_at, :n_steps_x, :data.shape[1]-1]\n",
    "X_test = input_data_shiftedy[val_split_at:, :n_steps_x, :data.shape[1]-1]\n",
    "\n",
    "Y = np.empty((input_data_shiftedy.shape[0], n_steps_x))\n",
    "for step_ahead in range(1, n_steps_y + 1):\n",
    "    # print(step_ahead, step_ahead + n_steps_x)\n",
    "    Y = input_data_shiftedy[..., step_ahead:step_ahead + n_steps_x, 0]\n",
    "Y_train = Y[:split_at, data.shape[1]-1:]\n",
    "Y_valid = Y[split_at:val_split_at,data.shape[1]-1:]\n",
    "Y_test = Y[val_split_at:,data.shape[1]-1:]\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1aa8eea-aa60-46e0-87c4-4a36b8e58b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EDmdl(X_train,Y_train,X_test,Y_test,X_valid,Y_valid):#,var):\n",
    "    optimizer = keras.optimizers.Adam(clipvalue = 1)\n",
    "    # simplified wavenet\n",
    "    def last_time_step_mse(Y_true, Y_pred):\n",
    "        return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = [X_train.shape[1], X_train.shape[2]]))\n",
    "    for rate in (1, 2, 4, 8, 16) * 2:\n",
    "        model.add(keras.layers.Conv1D(filters = 100, kernel_size = 2, padding = \"causal\",\n",
    "                                     activation = \"relu\", dilation_rate = rate))\n",
    "    model.add(keras.layers.Conv1D(filters = 1, kernel_size = 1))\n",
    "    # model.add(keras.layers.LSTM(100))\n",
    "    # model.add(keras.layers.Dense(1))\n",
    "    model.add(keras.layers.Lambda(lambda x: tf.reshape(x, [-1, 14])))\n",
    "    model.add(keras.layers.Dense(1)) ## to collapse all var to predict prevalence alone\n",
    "    model.compile(loss = \"mse\", optimizer = optimizer, metrics = [last_time_step_mse])\n",
    "\n",
    "#     history = model.fit(X_train, Y_train, epochs=30,verbose=0, shuffle=True, validation_data=(X_valid, Y_valid),callbacks = [callback])\n",
    "\n",
    "#     pred = model.predict(X_test)\n",
    "#     keras.backend.clear_session()\n",
    "#     # mse = (mean_squared_error(pred, Y_test))\n",
    "#     print('Test MSE: %.3f' % mse)\n",
    "\n",
    "#     plt.plot(np.double(pred[:, -1]-.5).flatten(), alpha = .5,label='pred')\n",
    "#     plt.plot(np.double(Y_test[:, -1]).flatten(), alpha = .5,label='valid')\n",
    "#     # plt.title(var+'_MSE: '+str(mse))\n",
    "#     plt.legend()\n",
    "#     plt.savefig('edNNtest/'+var+'_mse.png')\n",
    "    # print(model.summary())\n",
    "#     plt.clf()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7bbb89-1504-4357-9c37-9d57bf447efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(X_train,Y_train,X_test,Y_test,X_valid,Y_valid):\n",
    "    # use simple CNN structure\n",
    "    in_shape = ([X_train.shape[1], X_train.shape[2]])\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.ConvLSTM2D(32, kernel_size=(7, 7), padding='valid', return_sequences=True, input_shape=in_shape))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.ConvLSTM2D(64, kernel_size=(5, 5), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.ConvLSTM2D(96, kernel_size=(3, 3), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.ConvLSTM2D(96, kernel_size=(3, 3), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.ConvLSTM2D(96, kernel_size=(3, 3), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.Dense(320))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # out_shape = model.output_shape\n",
    "    # print('====Model shape: ', out_shape)\n",
    "    # model.add(Reshape((SequenceLength, out_shape[2] * out_shape[3] * out_shape[4])))\n",
    "    model.add(keras.layers.LSTM(64, return_sequences=False))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    # model structure summary\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6d3ed-5501-4500-9db9-ddb5f3d61742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1712aabf-7c4f-461a-8f14-90678eed516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1861, 14, 7) (1861, 7) (219, 14, 7) (219, 7) (233, 14, 7) (233, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 14, 7), found shape=(None, 14, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# model=load_model(train_X, train_Y,test_X,test_Y,valid_X,valid_Y)#,var)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m callback \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# keras.backend.clear_session()\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# mse = (mean_squared_error(pred, Y_test))\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mypy38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_h3ztdsi.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/dcmorgan/.conda/envs/mypy38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 14, 7), found shape=(None, 14, 6)\n"
     ]
    }
   ],
   "source": [
    "var_list = []\n",
    "mse_list = []\n",
    "# site_list = []\n",
    "order_list = []\n",
    "x_days=14\n",
    "y_days=1\n",
    "\n",
    "for i in ['all','ae','poll','prev']:\n",
    "    # for SS in ['orig','shuf']:\n",
    "\n",
    "    with open(\"edmond_datasets.pickle\", \"rb\") as handle:\n",
    "        input_data_shiftedy = pickle.load(handle)\n",
    "    # select only n, daily asthma visits and total, daily total AE visits, \n",
    "    # comment out if including air pollution station data\n",
    "    first_col = input_data_shiftedy.n / input_data_shiftedy.total * 1000\n",
    "    input_data_shiftedy.insert(0, \"prevalence\", first_col)\n",
    "    input_data_shiftedy = input_data_shiftedy.drop([\"n\", \"total\", \"y\"], axis = 1)\n",
    "    data=pd.DataFrame(input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1),columns=['CO'])\n",
    "    data['NO2']=input_data_shiftedy.filter(like='NO2_', axis=1).mean(axis=1)\n",
    "    data['O3']=input_data_shiftedy.filter(like='O3_', axis=1).mean(axis=1)\n",
    "    data['SO2']=input_data_shiftedy.filter(like='SO2_', axis=1).mean(axis=1)\n",
    "    data['CO']=input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1)\n",
    "    data['FSP']=input_data_shiftedy.filter(like='FSP_', axis=1).mean(axis=1)\n",
    "    data[['prevalence','temp','RelHum']]=input_data_shiftedy[['prevalence','Mean (deg. C)','Mean Relative Humidity (%)']]\n",
    "    # input_data_shiftedy = pd.concat([input_data_shiftedy.n.reset_index(drop = True), \n",
    "    #                         input_data_shiftedy.total.reset_index(drop = True)], axis = 1)\n",
    "    # input_data_shiftedy = input_data_shiftedy[var]\n",
    "    first_column = data.pop('prevalence')\n",
    "    data.insert(0, 'prevalence', first_column)\n",
    "    \n",
    "    # dataset=data\n",
    "    # values = dataset.values\n",
    "    # groups = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    # i = 1\n",
    "    # # plot each column\n",
    "    # plt.figure(figsize=(10,8))\n",
    "    # for group in groups:\n",
    "    #     plt.subplot(len(groups), 1, i)\n",
    "    #     plt.plot(values[:, group])\n",
    "    #     plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    #     i += 1\n",
    "    # plt.show()\n",
    "    \n",
    "    if i=='prev':\n",
    "        data=data[['prevalence']]\n",
    "    elif i=='ae':\n",
    "        data=data[['prevalence','temp','RelHum']]\n",
    "    elif i =='poll':\n",
    "        data=data[['prevalence','FSP','O3','NO2','SO2','CO']]\n",
    "    elif i =='all':\n",
    "        data=data\n",
    "    # n_days=7\n",
    "    if keras.Model:\n",
    "        keras.backend.clear_session()\n",
    "    train_X, train_Y,test_X,test_Y,valid_X,valid_Y,var=load_EDdata(data,i,x_days,y_days)\n",
    "    model=run_EDmdl(train_X, train_Y,test_X,test_Y,valid_X,valid_Y)#,var)\n",
    "    # model=load_model(train_X, train_Y,test_X,test_Y,valid_X,valid_Y)#,var)\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    history = model.fit(X_train, Y_train, epochs=30,verbose=0, shuffle=True, validation_data=(X_valid, Y_valid),callbacks = [callback])\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    # keras.backend.clear_session()\n",
    "    # mse = (mean_squared_error(pred, Y_test))\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "\n",
    "    plt.plot(np.double(pred[:, -1]-.5).flatten(), alpha = .5,label='pred')\n",
    "    plt.plot(np.double(Y_test[:, -1]).flatten(), alpha = .5,label='valid')\n",
    "    # plt.title(var+'_MSE: '+str(mse))\n",
    "    plt.legend()\n",
    "    plt.savefig('edNNtest/'+var+'_mse.png')\n",
    "    # plt.clf()\n",
    "    var_list.append(i)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "df = pd.DataFrame(list(zip(var_list , mse_list)),#,order_list)), \n",
    "           columns =['var', ',mse'])\n",
    "\n",
    "df.to_csv('NNtest/NN_mse.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0a202-2a73-46b2-a3ba-21acfbc1fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f317d4-ede2-4ba5-bae9-cd6304bd7e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37992bdd-2503-47ff-a593-9006b3a3a3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4d2c5-320d-4f73-b8cf-0a56a3889a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edmond_datasets.pickle\", \"rb\") as handle:\n",
    "    input_data_shiftedy = pickle.load(handle)\n",
    "\n",
    "first_col = input_data_shiftedy.n / input_data_shiftedy.total * 1000\n",
    "input_data_shiftedy.insert(0, \"prevalence\", first_col)\n",
    "input_data_shiftedy = input_data_shiftedy.drop([\"n\", \"total\", \"y\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61fd91-ca20-4d68-ba5f-0fc5616ba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1),columns=['CO'])\n",
    "data['NO2']=input_data_shiftedy.filter(like='NO2_', axis=1).mean(axis=1)\n",
    "data['O3']=input_data_shiftedy.filter(like='O3_', axis=1).mean(axis=1)\n",
    "data['SO2']=input_data_shiftedy.filter(like='SO2_', axis=1).mean(axis=1)\n",
    "data['CO']=input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1)\n",
    "data['FSP']=input_data_shiftedy.filter(like='FSP_', axis=1).mean(axis=1)\n",
    "data[['prevalence','temp','RelHum']]=input_data_shiftedy[['prevalence','Mean (deg. C)','Mean Relative Humidity (%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc927551-b7cd-4f94-b51c-1b9733c8e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09870ea3-9350-4268-a49e-2ce51eff5ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59837d65-ee82-441c-b036-fe8be44331d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16a47d-f9a9-4b6e-a1ec-6930cfaa4028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "packed-toyota",
   "metadata": {},
   "source": [
    "# modelling\n",
    "1. as of 23rd aug 2022: rnn loss nan problem <br>\n",
    "possible remedies: normalize data, regularization, increase batch size <br>\n",
    "https://datascience.stackexchange.com/questions/68331/keras-sequential-model-returns-loss-nan <br>\n",
    "2. as of 24th aug 2022: try to add overall AE visit to include hospital avoidance effect\n",
    "3. as of 25th aug 2022: corrected reshaping problem, performance is still shit, try remove air pollution, try larger learning rate\n",
    "4. as of 26th aug 2022: try seq2seq model, it worked \n",
    "5. from the results it seems most contributing factor is total AE trend \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-ocean",
   "metadata": {},
   "source": [
    "# shap value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return model.predict(X)[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:50, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/slundberg/shap/issues/1226 the nonetype shape problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-lottery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy38",
   "language": "python",
   "name": "mypy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
