{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# !pip install shap\n",
    "# import shap\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481d209-123d-4f73-bd9e-fa5598119bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sliding1(X, num_steps_x = 2, num_steps_y = 2):\n",
    "    X = pd.DataFrame(X)\n",
    "    X_transformed = [np.array(X.shift(i)) for i in range(num_steps_x + num_steps_y)]\n",
    "    X_transformed = np.dstack(X_transformed)\n",
    "    \n",
    "    # swap time steps and dimensionality axes\n",
    "    X_transformed = np.swapaxes(X_transformed, 1, 2)\n",
    "    # flip time steps axis\n",
    "    X_transformed = np.flip(X_transformed, 1)\n",
    "    X_transformed = X_transformed[(num_steps_x+num_steps_y - 1):]\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "super-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_EDdata(input_data_shiftedy,var,n_steps_x,n_steps_y):\n",
    "\n",
    "    # number of time steps for sequence learning\n",
    "    # n_steps_x = 14\n",
    "    # number of time steps for sequence prediction\n",
    "    # n_steps_y = 1\n",
    "\n",
    "    split_at = int(len(input_data_shiftedy) * .8)\n",
    "    val_split_at = int(len(input_data_shiftedy) * .9)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    # if only prevalence data\n",
    "    input_data_shiftedy = input_data_shiftedy.values.reshape((input_data_shiftedy.shape[0], -1))\n",
    "    input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)] = scaler.fit_transform(\n",
    "        input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)])\n",
    "    # only transform valid and testing sets\n",
    "    input_data_shiftedy[(split_at + n_steps_x + n_steps_y):] = scaler.transform(\n",
    "        input_data_shiftedy[(split_at + n_steps_x + n_steps_y):])\n",
    "\n",
    "\n",
    "    input_data_shiftedy = reshape_sliding1(input_data_shiftedy, \n",
    "                                           num_steps_x = n_steps_x,\n",
    "                                          num_steps_y = n_steps_y)\n",
    "\n",
    "    # train_test_split \n",
    "\n",
    "   \n",
    "    X_train = input_data_shiftedy[:split_at, :n_steps_x, 1]#:data.shape[1]-1]\n",
    "    X_valid = input_data_shiftedy[split_at:val_split_at, :n_steps_x, 1:]#data.shape[1]-1]\n",
    "    X_test = input_data_shiftedy[val_split_at:, :n_steps_x, 1]#:data.shape[1]-1]\n",
    "\n",
    "    Y = np.empty((input_data_shiftedy.shape[0], n_steps_x))\n",
    "    for step_ahead in range(1, n_steps_y + 1):\n",
    "        # print(step_ahead, step_ahead + n_steps_x)\n",
    "        Y = input_data_shiftedy[..., step_ahead:step_ahead + n_steps_x, 0]\n",
    "    Y_train = Y[:split_at, 0:1]\n",
    "    Y_valid = Y[split_at:val_split_at,0:1]\n",
    "    Y_test = Y[val_split_at:,0:1]\n",
    "\n",
    "    print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\n",
    "\n",
    "    return X_train,Y_train,X_test,Y_test,X_valid,Y_valid,var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f4f295d-2b49-434f-b415-5a0d8c437d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1861, 14, 7) (1861, 1) (219, 14, 7) (219, 1) (233, 14, 7) (233, 1)\n",
      "(1861, 14, 7) (1861, 1) (219, 14, 7) (219, 1) (233, 14, 7) (233, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y,test_X,test_Y,valid_X,valid_Y,var=load_EDdata(data,i,x_days,y_days)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02cc00ab-d53b-4b82-be1f-a86c5c18eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1861, 14, 7) (1861, 1) (219, 14, 7) (219, 1) (233, 14, 7) (233, 1)\n"
     ]
    }
   ],
   "source": [
    "n_steps_x=14\n",
    "n_steps_y=1\n",
    "\n",
    "with open(\"edmond_datasets.pickle\", \"rb\") as handle:\n",
    "    input_data_shiftedy = pickle.load(handle)\n",
    "# select only n, daily asthma visits and total, daily total AE visits, \n",
    "# comment out if including air pollution station data\n",
    "first_col = input_data_shiftedy.n / input_data_shiftedy.total * 1000\n",
    "input_data_shiftedy.insert(0, \"prevalence\", first_col)\n",
    "input_data_shiftedy = input_data_shiftedy.drop([\"n\", \"total\", \"y\"], axis = 1)\n",
    "data=pd.DataFrame(input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1),columns=['CO'])\n",
    "data['NO2']=input_data_shiftedy.filter(like='NO2_', axis=1).mean(axis=1)\n",
    "data['O3']=input_data_shiftedy.filter(like='O3_', axis=1).mean(axis=1)\n",
    "data['SO2']=input_data_shiftedy.filter(like='SO2_', axis=1).mean(axis=1)\n",
    "data['CO']=input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1)\n",
    "data['FSP']=input_data_shiftedy.filter(like='FSP_', axis=1).mean(axis=1)\n",
    "data[['prevalence','temp','RelHum']]=input_data_shiftedy[['prevalence','Mean (deg. C)','Mean Relative Humidity (%)']]\n",
    "# input_data_shiftedy = pd.concat([input_data_shiftedy.n.reset_index(drop = True), \n",
    "#                         input_data_shiftedy.total.reset_index(drop = True)], axis = 1)\n",
    "# input_data_shiftedy = input_data_shiftedy[var]\n",
    "first_column = data.pop('prevalence')\n",
    "data.insert(0, 'prevalence', first_column)\n",
    "\n",
    "# if i=='prev':\n",
    "#     data=data[['prevalence']]\n",
    "# elif i=='ae':\n",
    "#     data=data[['prevalence','temp','RelHum']]\n",
    "# elif i =='poll':\n",
    "input_data_shiftedy=data#[['prevalence','FSP','O3','NO2','SO2','CO']]\n",
    "\n",
    "split_at = int(len(input_data_shiftedy) * .8)\n",
    "val_split_at = int(len(input_data_shiftedy) * .9)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# if only prevalence data\n",
    "input_data_shiftedy = input_data_shiftedy.values.reshape((input_data_shiftedy.shape[0], -1))\n",
    "input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)] = scaler.fit_transform(\n",
    "    input_data_shiftedy[:(split_at + n_steps_x + n_steps_y)])\n",
    "# only transform valid and testing sets\n",
    "input_data_shiftedy[(split_at + n_steps_x + n_steps_y):] = scaler.transform(\n",
    "    input_data_shiftedy[(split_at + n_steps_x + n_steps_y):])\n",
    "\n",
    "\n",
    "input_data_shiftedy = reshape_sliding1(input_data_shiftedy, \n",
    "                                       num_steps_x = n_steps_x,\n",
    "                                      num_steps_y = n_steps_y)\n",
    "\n",
    "# data=input_data_shiftedy\n",
    "\n",
    "X_train = input_data_shiftedy[:split_at, :n_steps_x, 1:]#data.shape[1]-1]\n",
    "X_valid = input_data_shiftedy[split_at:val_split_at, :n_steps_x, 1:]#data.shape[1]-1]\n",
    "X_test = input_data_shiftedy[val_split_at:, :n_steps_x, 1:]#data.shape[1]-1]\n",
    "\n",
    "Y = np.empty((input_data_shiftedy.shape[0], n_steps_x))\n",
    "for step_ahead in range(1, n_steps_y + 1):\n",
    "    # print(step_ahead, step_ahead + n_steps_x)\n",
    "    Y = input_data_shiftedy[..., step_ahead:step_ahead + n_steps_x, 0]\n",
    "Y_train = Y[:split_at, 0:1]\n",
    "Y_valid = Y[split_at:val_split_at,0:1]\n",
    "Y_test = Y[val_split_at:,0:1]\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa8eea-aa60-46e0-87c4-4a36b8e58b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EDmdl(X_train,Y_train,X_test,Y_test,X_valid,Y_valid):#,var):\n",
    "    optimizer = keras.optimizers.Adam(clipvalue = 1)\n",
    "    # simplified wavenet\n",
    "    def last_time_step_mse(Y_true, Y_pred):\n",
    "        return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = [X_train.shape[1], X_train.shape[2]]))\n",
    "    for rate in (1, 2, 4, 8, 16) * 2:\n",
    "        model.add(keras.layers.Conv1D(filters = 100, kernel_size = 2, padding = \"causal\",\n",
    "                                     activation = \"relu\", dilation_rate = rate))\n",
    "    model.add(keras.layers.Conv1D(filters = 1, kernel_size = 1))\n",
    "    # model.add(keras.layers.LSTM(100))\n",
    "    # model.add(keras.layers.Dense(1))\n",
    "    model.add(keras.layers.Lambda(lambda x: tf.reshape(x, [-1, 14])))\n",
    "    model.add(keras.layers.Dense(1)) ## to collapse all var to predict prevalence alone\n",
    "    model.compile(loss = \"mse\", optimizer = optimizer, metrics = [last_time_step_mse])\n",
    "\n",
    "#     history = model.fit(X_train, Y_train, epochs=30,verbose=0, shuffle=True, validation_data=(X_valid, Y_valid),callbacks = [callback])\n",
    "\n",
    "#     pred = model.predict(X_test)\n",
    "#     keras.backend.clear_session()\n",
    "#     # mse = (mean_squared_error(pred, Y_test))\n",
    "#     print('Test MSE: %.3f' % mse)\n",
    "\n",
    "#     plt.plot(np.double(pred[:, -1]-.5).flatten(), alpha = .5,label='pred')\n",
    "#     plt.plot(np.double(Y_test[:, -1]).flatten(), alpha = .5,label='valid')\n",
    "#     # plt.title(var+'_MSE: '+str(mse))\n",
    "#     plt.legend()\n",
    "#     plt.savefig('edNNtest/'+var+'_mse.png')\n",
    "    # print(model.summary())\n",
    "#     plt.clf()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bbb89-1504-4357-9c37-9d57bf447efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(X_train,Y_train,X_test,Y_test,X_valid,Y_valid):\n",
    "    # use simple CNN structure\n",
    "    in_shape = ([X_train.shape[1], X_train.shape[2]])\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.ConvLSTM2D(32, kernel_size=(7, 7), padding='valid', return_sequences=True, input_shape=in_shape))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.ConvLSTM2D(64, kernel_size=(5, 5), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.ConvLSTM2D(96, kernel_size=(3, 3), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.ConvLSTM2D(96, kernel_size=(3, 3), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.ConvLSTM2D(96, kernel_size=(3, 3), padding='valid', return_sequences=True))\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.Dense(320))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # out_shape = model.output_shape\n",
    "    # print('====Model shape: ', out_shape)\n",
    "    # model.add(Reshape((SequenceLength, out_shape[2] * out_shape[3] * out_shape[4])))\n",
    "    model.add(keras.layers.LSTM(64, return_sequences=False))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    # model structure summary\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6d3ed-5501-4500-9db9-ddb5f3d61742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1712aabf-7c4f-461a-8f14-90678eed516e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mModel:\n\u001b[1;32m     54\u001b[0m     keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m---> 55\u001b[0m train_X, train_Y,test_X,test_Y,valid_X,valid_Y,var\u001b[38;5;241m=\u001b[39m\u001b[43mload_EDdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_days\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_days\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m=\u001b[39mrun_EDmdl(train_X, train_Y,test_X,test_Y,valid_X,valid_Y)\u001b[38;5;66;03m#,var)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36mload_EDdata\u001b[0;34m(input_data_shiftedy, var, n_steps_x, n_steps_y)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# train_test_split \u001b[39;00m\n\u001b[1;32m     29\u001b[0m X_train \u001b[38;5;241m=\u001b[39m input_data_shiftedy[:split_at, :n_steps_x, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;66;03m#:data.shape[1]-1]\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m X_valid \u001b[38;5;241m=\u001b[39m input_data_shiftedy[split_at:val_split_at, :n_steps_x, \u001b[38;5;241m1\u001b[39m:\u001b[43md\u001b[49m]\u001b[38;5;66;03m#ata.shape[1]-1]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_test \u001b[38;5;241m=\u001b[39m input_data_shiftedy[val_split_at:, :n_steps_x, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;66;03m#:data.shape[1]-1]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((input_data_shiftedy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_steps_x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "var_list = []\n",
    "mse_list = []\n",
    "# site_list = []\n",
    "order_list = []\n",
    "x_days=14\n",
    "y_days=1\n",
    "\n",
    "for i in ['all','poll','ae','prev']:\n",
    "    # for SS in ['orig','shuf']:\n",
    "\n",
    "    with open(\"edmond_datasets.pickle\", \"rb\") as handle:\n",
    "        input_data_shiftedy = pickle.load(handle)\n",
    "    # select only n, daily asthma visits and total, daily total AE visits, \n",
    "    # comment out if including air pollution station data\n",
    "    first_col = input_data_shiftedy.n / input_data_shiftedy.total * 1000\n",
    "    input_data_shiftedy.insert(0, \"prevalence\", first_col)\n",
    "    input_data_shiftedy = input_data_shiftedy.drop([\"n\", \"total\", \"y\"], axis = 1)\n",
    "    data=pd.DataFrame(input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1),columns=['CO'])\n",
    "    data['NO2']=input_data_shiftedy.filter(like='NO2_', axis=1).mean(axis=1)\n",
    "    data['O3']=input_data_shiftedy.filter(like='O3_', axis=1).mean(axis=1)\n",
    "    data['SO2']=input_data_shiftedy.filter(like='SO2_', axis=1).mean(axis=1)\n",
    "    data['CO']=input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1)\n",
    "    data['FSP']=input_data_shiftedy.filter(like='FSP_', axis=1).mean(axis=1)\n",
    "    data[['prevalence','temp','RelHum']]=input_data_shiftedy[['prevalence','Mean (deg. C)','Mean Relative Humidity (%)']]\n",
    "    # input_data_shiftedy = pd.concat([input_data_shiftedy.n.reset_index(drop = True), \n",
    "    #                         input_data_shiftedy.total.reset_index(drop = True)], axis = 1)\n",
    "    # input_data_shiftedy = input_data_shiftedy[var]\n",
    "    first_column = data.pop('prevalence')\n",
    "    data.insert(0, 'prevalence', first_column)\n",
    "    \n",
    "    # dataset=data\n",
    "    # values = dataset.values\n",
    "    # groups = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    # i = 1\n",
    "    # # plot each column\n",
    "    # plt.figure(figsize=(10,8))\n",
    "    # for group in groups:\n",
    "    #     plt.subplot(len(groups), 1, i)\n",
    "    #     plt.plot(values[:, group])\n",
    "    #     plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    #     i += 1\n",
    "    # plt.show()\n",
    "    \n",
    "    if i=='prev':\n",
    "        data=data[['prevalence']]\n",
    "    elif i=='ae':\n",
    "        data=data[['prevalence','temp','RelHum']]\n",
    "    elif i =='poll':\n",
    "        data=data[['prevalence','FSP','O3','NO2','SO2','CO']]\n",
    "    elif i =='all':\n",
    "        data=data\n",
    "    # n_days=7\n",
    "    if keras.Model:\n",
    "        keras.backend.clear_session()\n",
    "    train_X, train_Y,test_X,test_Y,valid_X,valid_Y,var=load_EDdata(data,i,x_days,y_days)\n",
    "    # print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\n",
    "\n",
    "    model=run_EDmdl(train_X, train_Y,test_X,test_Y,valid_X,valid_Y)#,var)\n",
    "    # model=load_model(train_X, train_Y,test_X,test_Y,valid_X,valid_Y)#,var)\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    history = model.fit(X_train, Y_train, epochs=30,verbose=0, shuffle=True, validation_data=(X_valid, Y_valid),callbacks = [callback])\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    # keras.backend.clear_session()\n",
    "    mse = (mean_squared_error(pred, Y_test))\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    keras.backend.clear_session()\n",
    "    plt.plot(np.double(pred[:, -1]-.5).flatten(), alpha = .5,label='pred')\n",
    "    plt.plot(np.double(Y_test[:, -1]).flatten(), alpha = .5,label='valid')\n",
    "    # plt.title(var+'_MSE: '+str(mse))\n",
    "    plt.legend()\n",
    "    plt.savefig('edNNtest/'+var+'_mse.png')\n",
    "    # plt.clf()\n",
    "    var_list.append(i)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "df = pd.DataFrame(list(zip(var_list , mse_list)),#,order_list)), \n",
    "           columns =['var', ',mse'])\n",
    "\n",
    "df.to_csv('NNtest/NN_mse.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e388aaea-c77f-4e1f-975d-dbc198239df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.Model\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fb0a202-2a73-46b2-a3ba-21acfbc1fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1861, 14, 8) (1861, 0) (219, 14, 8) (219, 0) (233, 14, 8) (233, 0)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape,X_valid.shape, Y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37992bdd-2503-47ff-a593-9006b3a3a3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4d2c5-320d-4f73-b8cf-0a56a3889a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edmond_datasets.pickle\", \"rb\") as handle:\n",
    "    input_data_shiftedy = pickle.load(handle)\n",
    "\n",
    "first_col = input_data_shiftedy.n / input_data_shiftedy.total * 1000\n",
    "input_data_shiftedy.insert(0, \"prevalence\", first_col)\n",
    "input_data_shiftedy = input_data_shiftedy.drop([\"n\", \"total\", \"y\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61fd91-ca20-4d68-ba5f-0fc5616ba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1),columns=['CO'])\n",
    "data['NO2']=input_data_shiftedy.filter(like='NO2_', axis=1).mean(axis=1)\n",
    "data['O3']=input_data_shiftedy.filter(like='O3_', axis=1).mean(axis=1)\n",
    "data['SO2']=input_data_shiftedy.filter(like='SO2_', axis=1).mean(axis=1)\n",
    "data['CO']=input_data_shiftedy.filter(like='CO_', axis=1).mean(axis=1)\n",
    "data['FSP']=input_data_shiftedy.filter(like='FSP_', axis=1).mean(axis=1)\n",
    "data[['prevalence','temp','RelHum']]=input_data_shiftedy[['prevalence','Mean (deg. C)','Mean Relative Humidity (%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc927551-b7cd-4f94-b51c-1b9733c8e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09870ea3-9350-4268-a49e-2ce51eff5ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59837d65-ee82-441c-b036-fe8be44331d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16a47d-f9a9-4b6e-a1ec-6930cfaa4028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "packed-toyota",
   "metadata": {},
   "source": [
    "# modelling\n",
    "1. as of 23rd aug 2022: rnn loss nan problem <br>\n",
    "possible remedies: normalize data, regularization, increase batch size <br>\n",
    "https://datascience.stackexchange.com/questions/68331/keras-sequential-model-returns-loss-nan <br>\n",
    "2. as of 24th aug 2022: try to add overall AE visit to include hospital avoidance effect\n",
    "3. as of 25th aug 2022: corrected reshaping problem, performance is still shit, try remove air pollution, try larger learning rate\n",
    "4. as of 26th aug 2022: try seq2seq model, it worked \n",
    "5. from the results it seems most contributing factor is total AE trend \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-ocean",
   "metadata": {},
   "source": [
    "# shap value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return model.predict(X)[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:50, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/slundberg/shap/issues/1226 the nonetype shape problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-lottery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy38",
   "language": "python",
   "name": "mypy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
