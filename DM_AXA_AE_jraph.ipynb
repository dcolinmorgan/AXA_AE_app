{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcolinmorgan/AXA_AE_app/blob/main/DM_AXA_AE_jraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUxD4gOd-AuO"
   },
   "source": [
    "## Setup: Install and Import libraries\n",
    "[modified from tutorial HERE](https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb) |  \n",
    "[merge with this](https://keras.io/examples/timeseries/) |   [also this](https://github.com/deepmind/graph_nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OyH_VgjW7Bp3"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/deepmind/jraph.git\n",
    "!pip install flax\n",
    "!pip install dm-haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RJm7y6GH3WyB"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import functools,glob,jax,jraph,optax,pickle,flax,os,collections,gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as tree\n",
    "import haiku as hk\n",
    "import absl as app\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopy.distance\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "StatefulField = collections.namedtuple(\"StatefulField\", [\"embedding\", \"state\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SE5DQoXWQJR"
   },
   "source": [
    "Let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt_f14MEbPxb"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/dcolinmorgan/AXA_AE_app/raw/main/axa_p.zip\n",
    "# !7z x /content/axa_p.zip\n",
    "# !git clone https://github.com/dcolinmorgan/aqi-stations-scraper.git\n",
    "\n",
    "df2=pd.read_parquet('/content/AE_AXA_dat_full.parquet')\n",
    "\n",
    "\n",
    "# list_a = ['pneumonia', 'COPD','asthma','resp','lung','pulm']#,'~Cancer']\n",
    "# list_b = ['Cancer']\n",
    "df2.columns=['pat_id','cd9_loc','sess','sex','age','cd9_code','mini_loc','loc1','date','tmp','diag1','diag2','tmp']\n",
    "\n",
    "# df2=df[df['diag1'].isin(list_a)]\n",
    "# df2 = df[df['diag1'].str.contains('|'.join(list_a))]\n",
    "\n",
    "# df.diag1, df.diag2 = np.where(df.diag1.str.contains('None'), [df.diag2, df.diag1], [df.diag1, df.diag2])\n",
    "# del df['sess'], df['tmp'], df['diag2']\n",
    "# df=df[~df['diag1'].isna()]\n",
    "\n",
    "# df2 = df[df['diag1'].str.contains('|'.join(list_a))]\n",
    "\n",
    "df2.replace({'RH':'Ruttonjee Hospital'},inplace=True)\n",
    "df2.replace({'PYN':'Pamela Youde Nethersole Eastern Hospital'},inplace=True)\n",
    "df2.replace({'QEH':'Queen Elizabeth Hospital'},inplace=True)\n",
    "df2.replace({'CMC':'Caritas Medical Centre'},inplace=True)\n",
    "df2.replace({'KWH':'Kwong Wah Hospital'},inplace=True)\n",
    "df2.replace({'TMH':'Tuen Mun Hospital'},inplace=True)\n",
    "df2.replace({'PWH':'Prince of Wales Hospital'},inplace=True)\n",
    "df2.replace({'NDH':'North District Hospital'},inplace=True)\n",
    "df2.replace({'YCH':'Yan Chai Hospital'},inplace=True)\n",
    "df2.replace({'UCH':'United Christian Hospital'},inplace=True)\n",
    "df2.replace({'QMH':'Queen Mary Hospital'},inplace=True)\n",
    "df2.replace({'PWH':'Princess Margaret Hospital'},inplace=True)\n",
    "df2.replace({'POH':'Pok Oi Hospital'},inplace=True)\n",
    "df2.replace({'TKO':'Tseung Kwan O Hospital'},inplace=True)\n",
    "df2.replace({'AHN':'Alice Ho Miu Ling Nethersole Hospital'},inplace=True)\n",
    "df2.replace({'SJH':'St. John Hospital'},inplace=True)\n",
    "df2.replace({'NLT':'North Lantau Hospital'},inplace=True)\n",
    "df2.replace({'TSH':'Tang Shiu Kin Hospital'},inplace=True)\n",
    "df2.replace({'PMH':'Princess Margaret Hospital'},inplace=True)\n",
    "\n",
    "\n",
    "#organize\n",
    "cc=pd.DataFrame()#(columns=['date','pm25','pm10','o3','no2','so2','co','loc'])\n",
    "files=glob.glob('/content/aqi-stations-scraper/data/japan-aqi/*')\n",
    "for file in files:\n",
    "    data=pd.read_csv(file,sep=' |,')\n",
    "    data['loc1']=os.path.basename(file).split(',')[0]\n",
    "    cc=cc.append(data)\n",
    "\n",
    "data2=cc[['date','pm25','pm10','o3','no2','so2','co','loc1']]\n",
    "data2['loc1']=data2['loc1'].str.upper().replace({'-':' '},regex=True)\n",
    "data2['date']=pd.to_datetime(data2['date'])\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"example app\")\n",
    "df_loc=pd.DataFrame(columns=['lat','long','name'])\n",
    "for ii,i in enumerate(pd.unique(df2['cd9_loc'])):\n",
    "    a,b,c=geolocator.geocode(str(i)+\", Hong Kong\").point\n",
    "    df_loc[ii]=[a,b,i]\n",
    "df_loc=df_loc.transpose()\n",
    "df_loc.columns=['lat','long','name']\n",
    "df_loc=df_loc[3:]\n",
    "\n",
    "\n",
    "data2.replace('CENTRALNAYA STR','central',inplace=True)\n",
    "data2.replace('SOUTHERN','southern island',inplace=True)\n",
    "data2.replace('SOUTHERN PART OF CHENGYANG DISTRICT','chengyang district',inplace=True)\n",
    "\n",
    "data_loc=pd.DataFrame(columns=['lat','long','name'])\n",
    "for ii,i in enumerate(pd.unique(data2['loc1'])):\n",
    "    try:\n",
    "        a,b,c=geolocator.geocode(str(i)+\", Hong Kong\").point\n",
    "    except AttributeError:\n",
    "        print('no location data for: '+str(i))\n",
    "    data_loc[ii]=[a,b,i]\n",
    "data_loc=data_loc.transpose()\n",
    "data_loc.columns=['lat','long','name']\n",
    "data_loc=data_loc[3:]\n",
    "\n",
    "data_loc=data_loc[~data_loc.duplicated(['lat','long'],keep='first')]\n",
    "data_loc.reset_index(inplace=True)\n",
    "\n",
    "data_loc=df_loc.append(data_loc)[['lat','long','name']]\n",
    "2\n",
    "data_loc.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# geopy DOES use latlon configuration\n",
    "data_loc['latlon'] = list(zip(data_loc['lat'], data_loc['long']))\n",
    "square = pd.DataFrame(\n",
    "    np.zeros((data_loc.shape[0], data_loc.shape[0])),\n",
    "    index=data_loc.index, columns=data_loc.index\n",
    ")\n",
    "\n",
    "# replacing distance.vicenty with distance.distance\n",
    "def get_distance(col):\n",
    "    end = data_loc.loc[col.name, 'latlon']\n",
    "    return data_loc['latlon'].apply(geopy.distance.distance,\n",
    "                              args=(end,),\n",
    "                              ellipsoid='WGS-84'\n",
    "                             )\n",
    "\n",
    "distances = square.apply(get_distance, axis=1).T\n",
    "\n",
    "data_loc['src']=data_loc['name']\n",
    "data_loc['dst']=data_loc['name']\n",
    "\n",
    "# np.sum((distances<5)*1)\n",
    "D_D=pd.DataFrame((distances<5)*1)\n",
    "D_D.index=data_loc['src']\n",
    "D_D.columns=data_loc['dst']\n",
    "\n",
    "E_E=pd.DataFrame(D_D.stack())#.reset_index(inplace=True)\n",
    "# E_E.rename=['source','target']#.reset_index(inplace=True)#.rename(columns={'level_0':'Source','level_1':'Target', 0:'Weight'})\n",
    "E_E.reset_index(inplace=True)#\n",
    "distance_mat=E_E[E_E[0]>0]\n",
    "\n",
    "distance=distances\n",
    "distance.index=data_loc['src']\n",
    "distance.columns=data_loc['dst']\n",
    "distance=pd.DataFrame(distance.stack())\n",
    "distance.reset_index(inplace=True)\n",
    "\n",
    "#prepare for TF\n",
    "\n",
    "distances=distances.astype(str) # df.astype(np.float64)#lues.as_int#('int')#.to_numpy()\n",
    "distances=distances.replace('km', '', regex=True)\n",
    "distances=distances.astype(np.float64)\n",
    "\n",
    "distances.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r4kiJ38ERDV"
   },
   "source": [
    "*n.b. set graph to share lung scores (binary) based on proxmity (ie share health scores to weather regions)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZSMpZtL8UL9",
    "outputId": "3528e14b-927f-4766-b291-68b908222098"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS1d=(df2.pivot_table(values='diag1', index='date', columns='cd9_loc', aggfunc='count').fillna(0).iloc[0].values)\n",
    "GS1d=(np.repeat(GS1d,2))\n",
    "# GS1d=GS1d[:-1]\n",
    "GS1d=(GS1d>0)*1\n",
    "GS1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e7q5ySSmVL3x"
   },
   "outputs": [],
   "source": [
    "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
    "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
    "  nx_graph = nx.DiGraph()\n",
    "  if nodes is None:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n)\n",
    "  else:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n, node_feature=nodes[n])\n",
    "  if edges is None:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
    "  else:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(\n",
    "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
    "  return nx_graph\n",
    "\n",
    "\n",
    "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
    "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
    "  pos = nx.spring_layout(nx_graph)\n",
    "  nx.draw(\n",
    "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')\n",
    "  \n",
    "def get_zacharys_karate_club(s_graph) -> jraph.GraphsTuple:\n",
    "  \"\"\"Returns GraphsTuple representing Zachary's karate club.\"\"\"\n",
    "\n",
    "  # social_graph += [(edge[1], edge[0]) for edge in social_graph]\n",
    "  n_nodes = np.max(s_graph)+1\n",
    "\n",
    "\n",
    "  return jraph.GraphsTuple(\n",
    "      n_node=jnp.asarray([n_nodes]),\n",
    "      n_edge=jnp.asarray([len(s_graph)]),\n",
    "      # One-hot encoding for nodes, i.e. argmax(nodes) = node index.\n",
    "      nodes=jnp.eye(n_nodes),\n",
    "      # No edge features.\n",
    "      edges=jnp.asarray(s_graph),#None,\n",
    "      globals=None,\n",
    "      senders=jnp.asarray([edge[0] for edge in s_graph]),\n",
    "      receivers=jnp.asarray([edge[1] for edge in s_graph]))\n",
    "\n",
    "def get_ground_truth_assignments_for_zacharys_karate_club(GS1d) -> jnp.ndarray:\n",
    "  \"\"\"Returns ground truth assignments for Zachary's karate club.\"\"\"\n",
    "  return jnp.array(GS1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyJk-Mq7EKoU",
    "outputId": "55f70879-080f-44fd-cad9-ca3fad8cf3aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "distances = square.apply(get_distance, axis=1).T\n",
    "D_D=pd.DataFrame((distances<5)*1)\n",
    "D_D.index=np.arange(0,D_D.shape[1])#data_loc['src']\n",
    "D_D.columns=np.arange(0,D_D.shape[1])#data_loc['dst']\n",
    "\n",
    "E_E=pd.DataFrame(D_D.stack())#.reset_index(inplace=True)\n",
    "E_E.reset_index(inplace=True)#\n",
    "# distance#_mat=E_E[E_E[0]>0]\n",
    "E_E=E_E[E_E['level_0']!=E_E['level_1']]\n",
    "s_graph=E_E[E_E[0]>0][['level_0','level_1']].to_numpy()#=distance_mat[['level_0','level_1']].to_numpy()\n",
    "# n_nodes=36\n",
    "\n",
    "graph = get_zacharys_karate_club(s_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KJqT6mjgBBmQ"
   },
   "outputs": [],
   "source": [
    "# GAT implementation adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L442.\n",
    "def GAT(attention_query_fn: Callable,\n",
    "        attention_logit_fn: Callable,\n",
    "        node_update_fn: Optional[Callable] = None,\n",
    "        add_self_edges: bool = True) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Attention Network layer.\n",
    "\n",
    "  Graph Attention message passing as described in\n",
    "  https://arxiv.org/pdf/1710.10903.pdf. This model expects node features as a\n",
    "  jnp.array, may use edge features for computing attention weights, and\n",
    "  ignore global features. It does not support nests.\n",
    "  Args:\n",
    "    attention_query_fn: function that generates attention queries from sender\n",
    "      node features.\n",
    "    attention_logit_fn: function that converts attention queries into logits for\n",
    "      softmax attention.\n",
    "    node_update_fn: function that updates the aggregated messages. If None, will\n",
    "      apply leaky relu and concatenate (if using multi-head attention).\n",
    "\n",
    "  Returns:\n",
    "    A function that applies a Graph Attention layer.\n",
    "  \"\"\"\n",
    "  # pylint: disable=g-long-lambda\n",
    "  if node_update_fn is None:\n",
    "    # By default, apply the leaky relu and then concatenate the heads on the\n",
    "    # feature axis.\n",
    "    node_update_fn = lambda x: jnp.reshape(\n",
    "        jax.nn.leaky_relu(x), (x.shape[0], -1))\n",
    "\n",
    "  def _ApplyGAT(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Attention layer.\"\"\"\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "    # Equivalent to the sum of n_node, but statically known.\n",
    "    try:\n",
    "      sum_n_node = nodes.shape[0]\n",
    "    except IndexError:\n",
    "      raise IndexError('GAT requires node features')\n",
    "\n",
    "    # Pass nodes through the attention query function to transform\n",
    "    # node features, e.g. with an MLP.\n",
    "    nodes = attention_query_fn(nodes)\n",
    "\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      receivers, senders = add_self_edges_fn(receivers, senders,\n",
    "                                             total_num_nodes)\n",
    "\n",
    "    # We compute the softmax logits using a function that takes the\n",
    "    # embedded sender and receiver attributes.\n",
    "    sent_attributes = nodes[senders]\n",
    "    received_attributes = nodes[receivers]\n",
    "    att_softmax_logits = attention_logit_fn(sent_attributes,\n",
    "                                            received_attributes, edges)\n",
    "\n",
    "    # Compute the attention softmax weights on the entire tree.\n",
    "    att_weights = jraph.segment_softmax(\n",
    "        att_softmax_logits, segment_ids=receivers, num_segments=sum_n_node)\n",
    "\n",
    "    # Apply attention weights.\n",
    "    messages = sent_attributes * att_weights\n",
    "    # Aggregate messages to nodes.\n",
    "    nodes = jax.ops.segment_sum(messages, receivers, num_segments=sum_n_node)\n",
    "\n",
    "    # Apply an update function to the aggregated messages.\n",
    "    nodes = node_update_fn(nodes)\n",
    "\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  # pylint: enable=g-long-lambda\n",
    "  return _ApplyGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jUXJcRnVMr4X"
   },
   "outputs": [],
   "source": [
    "def gat_definition(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a GAT network for the karate club node classification task.\n",
    "\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "\n",
    "  def _attention_logit_fn(sender_attr: jnp.ndarray, receiver_attr: jnp.ndarray,\n",
    "                          edges: jnp.ndarray) -> jnp.ndarray:\n",
    "    del edges\n",
    "    x = jnp.concatenate((sender_attr, receiver_attr), axis=1)\n",
    "    return hk.Linear(1)(x)\n",
    "\n",
    "  gn = GAT(\n",
    "      attention_query_fn=lambda n: hk.Linear(8)(n),\n",
    "      attention_logit_fn=_attention_logit_fn,\n",
    "      node_update_fn=None,\n",
    "      add_self_edges=True)\n",
    "  graph = gn(graph)\n",
    "\n",
    "  gn = GAT(\n",
    "      attention_query_fn=lambda n: hk.Linear(8)(n),\n",
    "      attention_logit_fn=_attention_logit_fn,\n",
    "      node_update_fn=hk.Linear(2),\n",
    "      add_self_edges=True)\n",
    "  graph = gn(graph)\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1n1kCuqtkvfm"
   },
   "outputs": [],
   "source": [
    "# def optimize_club(network: hk.Transformed, num_steps: int) -> jnp.ndarray:\n",
    "#   \"\"\"Solves the karate club problem by optimizing the assignments of students.\"\"\"\n",
    "#   zacharys_karate_club = get_zacharys_karate_club(s_graph)\n",
    "#   labels = get_ground_truth_assignments_for_zacharys_karate_club(GS1d)\n",
    "#   params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
    "\n",
    "@jax.jit\n",
    "def predict(params: hk.Params) -> jnp.ndarray:\n",
    "  decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "  return jnp.argmax(decoded_graph.nodes, axis=1)\n",
    "\n",
    "@jax.jit\n",
    "def prediction_loss(params: hk.Params) -> jnp.ndarray:\n",
    "  decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "  # We interpret the decoded nodes as a pair of logits for each node.\n",
    "  log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
    "  # The only two assignments we know a-priori are those of Mr. Hi (Node 0)\n",
    "  # and John A (Node 33).\n",
    "  return -(log_prob[0, 0] + log_prob[33, 1])\n",
    "\n",
    "opt_init, opt_update = optax.adam(1e-2)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "@jax.jit\n",
    "def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "  \"\"\"Returns updated params and state.\"\"\"\n",
    "  g = jax.grad(prediction_loss)(params)\n",
    "  updates, opt_state = opt_update(g, opt_state)\n",
    "  return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "@jax.jit\n",
    "def accuracy(params: hk.Params) -> jnp.ndarray:\n",
    "  decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "  return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
    "\n",
    "# for step in range(num_steps):\n",
    "#   print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
    "#   params, opt_state = update(params, opt_state)\n",
    "\n",
    "  # return predict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0leRi5igUUyd",
    "outputId": "1869e2c0-4196-4444-b804-0dca0615e855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 accuracy 0.67\n",
      "step 1 accuracy 0.50\n",
      "step 2 accuracy 0.39\n",
      "step 3 accuracy 0.44\n",
      "step 4 accuracy 0.50\n",
      "step 5 accuracy 0.50\n",
      "step 6 accuracy 0.53\n",
      "step 7 accuracy 0.53\n",
      "step 8 accuracy 0.50\n",
      "step 9 accuracy 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "             0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = hk.without_apply_rng(hk.transform(gat_definition))\n",
    "\n",
    "zacharys_karate_club = get_zacharys_karate_club(s_graph)\n",
    "labels = get_ground_truth_assignments_for_zacharys_karate_club(GS1d)\n",
    "params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
    "# # paramsA=params\n",
    "\n",
    "opt_init, opt_update = optax.adam(1e-2)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "for step in range(10):\n",
    "  print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
    "  params, opt_state = update(params, opt_state)\n",
    "\n",
    "jnp.argmax(network.apply(params, zacharys_karate_club).nodes,axis=1)\n",
    "# output_graph = network.apply(c, input_graph)\n",
    "\n",
    "\n",
    "\n",
    "# network = hk.without_apply_rng(hk.transform(network_definition))\n",
    "# input_graph = get_random_graph()\n",
    "# params = network.init(jax.random.PRNGKey(42), input_graph)\n",
    "# output_graph = network.apply(params, input_graph)\n",
    "# print(tree.tree_map(lambda x: x.shape, output_graph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "eBTzR082CoKS"
   },
   "outputs": [],
   "source": [
    "NET=network.apply(params, zacharys_karate_club)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKiTfJEi5_rQ"
   },
   "source": [
    "lets add in LSTM to model to account for time\n",
    "\n",
    "[inspired by GRU then LSTM](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/timeseries/ipynb/timeseries_traffic_forecasting.ipynb#scrollTo=hRpthm4r4j6a) | [and from googleBrain code here](https://github.com/deepmind/jraph/blob/master/jraph/examples/lstm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "-NePMdah55eB"
   },
   "outputs": [],
   "source": [
    "# NUM_NODES = 5\n",
    "# NUM_EDGES = 7\n",
    "# NUM_MESSAGE_PASSING_STEPS = 10\n",
    "# EMBEDDING_SIZE = 32\n",
    "# HIDDEN_SIZE = 128\n",
    "\n",
    "# Immutable class for storing nested node/edge features containing an embedding\n",
    "# and a recurrent state.\n",
    "StatefulField = collections.namedtuple(\"StatefulField\", [\"embedding\", \"state\"])\n",
    "\n",
    "\n",
    "# def get_random_graph() -> jraph.GraphsTuple:\n",
    "#   return jraph.GraphsTuple(\n",
    "#       n_node=np.asarray([NUM_NODES]),\n",
    "#       n_edge=np.asarray([NUM_EDGES]),\n",
    "#       nodes=np.random.normal(size=[NUM_NODES, EMBEDDING_SIZE]),\n",
    "#       edges=np.random.normal(size=[NUM_EDGES, EMBEDDING_SIZE]),\n",
    "#       globals=None,\n",
    "#       senders=np.random.randint(0, NUM_NODES, [NUM_EDGES]),\n",
    "#       receivers=np.random.randint(0, NUM_NODES, [NUM_EDGES]))\n",
    "\n",
    "\n",
    "def network_definition(graph: jraph.GraphsTuple) -> jraph.ArrayTree:\n",
    "  \"\"\"`InteractionNetwork` with an LSTM in the edge update.\"\"\"\n",
    "\n",
    "  # LSTM that will keep a memory of the inputs to the edge model.\n",
    "  edge_fn_lstm = hk.LSTM(hidden_size=128)\n",
    "\n",
    "  # MLPs used in the edge and the node model. Note that in this instance\n",
    "  # the output size matches the input size so the same model can be run\n",
    "  # iteratively multiple times. In a real model, this would usually be achieved\n",
    "  # by first using an encoder in the input data into a common `EMBEDDING_SIZE`.\n",
    "  edge_fn_mlp = hk.nets.MLP([128, 2])\n",
    "  node_fn_mlp = hk.nets.MLP([128, 2])\n",
    "\n",
    "  # Initialize the edge features to contain both the input edge embedding\n",
    "  # and initial LSTM state. Note for the nodes we only have an embedding since\n",
    "  # in this example nodes do not use a `node_fn_lstm`, but for analogy, we\n",
    "  # still put it in a `StatefulField`.\n",
    "  graph = graph._replace(\n",
    "      edges=StatefulField(\n",
    "          embedding=graph.edges,\n",
    "          state=edge_fn_lstm.initial_state(graph.edges.shape[0])),\n",
    "      nodes=StatefulField(embedding=graph.nodes, state=None),\n",
    "  )\n",
    "\n",
    "  def update_edge_fn(edges, sender_nodes, receiver_nodes):\n",
    "    # We will run an LSTM memory on the inputs first, and then\n",
    "    # process the output of the LSTM with an MLP.\n",
    "    edge_inputs = jnp.concatenate([edges.embedding,\n",
    "                                   sender_nodes.embedding,\n",
    "                                   receiver_nodes.embedding], axis=-1)\n",
    "    lstm_output, updated_state = edge_fn_lstm(edge_inputs, edges.state)\n",
    "    updated_edges = StatefulField(\n",
    "        embedding=edge_fn_mlp(lstm_output), state=updated_state,\n",
    "    )\n",
    "    return updated_edges\n",
    "\n",
    "  def update_node_fn(nodes, received_edges):\n",
    "    # Note `received_edges.state` will also contain the aggregated state for\n",
    "    # all received edges, which we may choose to use in the node update.\n",
    "    node_inputs = jnp.concatenate(\n",
    "        [nodes.embedding, received_edges.embedding], axis=-1)\n",
    "    updated_nodes = StatefulField(\n",
    "        embedding=node_fn_mlp(node_inputs),\n",
    "        state=None)\n",
    "    return updated_nodes\n",
    "\n",
    "  recurrent_graph_network = jraph.InteractionNetwork(\n",
    "      update_edge_fn=update_edge_fn,\n",
    "      update_node_fn=update_node_fn)\n",
    "\n",
    "  # Apply the model recurrently for 10 message passing steps.\n",
    "  # If instead we intended to use the LSTM to process a sequence of features\n",
    "  # for each node/edge, here we would select the corresponding inputs from the\n",
    "  # sequence along the sequence axis of the nodes/edges features to build the\n",
    "  # correct input graph for each step of the iteration.\n",
    "  num_message_passing_steps = 10\n",
    "  for _ in range(num_message_passing_steps):\n",
    "    graph = recurrent_graph_network(graph)\n",
    "\n",
    "  return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbv7l05D55gh",
    "outputId": "42d739ae-ca4e-4ad8-85bc-fcdc8004d7ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphsTuple(nodes=StatefulField(embedding=(36, 2), state=None), edges=StatefulField(embedding=(144, 2), state=LSTMState(hidden=(144, 128), cell=(144, 128))), receivers=(144,), senders=(144,), globals=None, n_node=(1,), n_edge=(1,))\n"
     ]
    }
   ],
   "source": [
    "network = hk.without_apply_rng(hk.transform(network_definition))\n",
    "# input_graph = get_random_graph()\n",
    "params = network.init(jax.random.PRNGKey(42), NET)\n",
    "output_graph = network.apply(params, input_graph)\n",
    "print(tree.tree_map(lambda x: x.shape, output_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "XNydIyWGJE4E"
   },
   "outputs": [],
   "source": [
    "CC=pd.DataFrame(output_graph.edges.embedding)\n",
    "CC.columns=['src','dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntKaMyG7JHKE"
   },
   "outputs": [],
   "source": [
    "!pip install graphistry\n",
    "import graphistry\n",
    "graphistry.register(api=3,protocol=\"https\", server=\"hub.graphistry.com\", username='dcolinmorgan', password='f5UwthGEF@F@xnP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "FK4mff0OJ3-v",
    "outputId": "4828d194-e5ac-4dc6-a3c2-73cfff84284e"
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-7601a4b5cd86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'src'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dst'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graphistry/PlotterBase.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, graph, nodes, name, description, render, skip_upload, as_files, memoize, extra_html, override_html_style)\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mPyGraphistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'arrow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_upload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graphistry/PlotterBase.py\u001b[0m in \u001b[0;36m_plot_dispatch\u001b[0;34m(self, graph, nodes, name, description, mode, metadata, memoize)\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaybe_dask_dataframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_dask_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaybe_spark\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graphistry/PlotterBase.py\u001b[0m in \u001b[0;36m_make_dataset\u001b[0;34m(self, edges, nodes, name, description, mode, metadata, memoize)\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_json_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'arrow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1895\u001b[0;31m             \u001b[0medges_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table_to_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1896\u001b[0m             \u001b[0mnodes_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table_to_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_arrow_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medges_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/graphistry/PlotterBase.py\u001b[0m in \u001b[0;36m_table_to_arrow\u001b[0;34m(self, table, memoize)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                     \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_schema_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmemoize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhashed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         arrays = [convert_column(c, f)\n\u001b[0;32m--> 595\u001b[0;31m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         arrays = [convert_column(c, f)\n\u001b[0;32m--> 595\u001b[0;31m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    579\u001b[0m             e.args += (\"Conversion failed for column {!s} with type {!s}\"\n\u001b[1;32m    580\u001b[0m                        .format(col.name, col.dtype),)\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfield_nullable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Field {} was non-nullable but pandas column \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         except (pa.ArrowInvalid,\n\u001b[1;32m    577\u001b[0m                 \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: ('Could not convert DeviceArray(0.00132459, dtype=float32) with type DeviceArray: did not recognize Python value type when inferring an Arrow data type', 'Conversion failed for column src with type object')"
     ]
    }
   ],
   "source": [
    "g = graphistry.edges(CC, 'src', 'dst')\n",
    "g.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GydJ1H5RL08U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DM_AXA_AE_jraph",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
